---
title: "Model checking and comparison"
author: "TP"
date: "14 10 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Bayesian workflow includes fitting many models and, naturally, we would like to evaluate and compare these models.
Without waiting for the new data, we can compare predictions on the observed data, but as we have already used this data to fit our models, the result is overly optimistic about our model performance.
In cross-validation (CV) proportion of the data is left out from model fitting and can be used later to assess model performance.
When prediction is not the goal of model building, CV can be means to assess generalization from one part of the data to another part.

There are different CV strategies, but in all cases the model is refitted when some part of the data is left out.
CV removes the over-fitting problem arising when model is fitted and evaluated on same exact data.

# Leave-one-out CV

In case leave-one-out CV model is fitted *n* times by leaving each time out one observation.
Obviously, this can be computation intensive with large data.

As described in "Regression and Other Stories" by A. Gelman et al., Bayesian approach to speed-up CV takes advantage of Bayesian inference, where posterior distribution can be written as the prior distribution multiplied by likelihood.

If the observations are conditionally independent, then the likelihood is the product of likelihoods of individual data points: $$p(\theta\vert y) \varpropto p(\theta) \prod^n_{i=1}p(y_i\vert\theta)$$

In this expression, leaving out observation *i* is equivalent to multiplying the posterior distribution by the factor $$1/p(y_i\vert\theta)$$


The LOO posterior excluding point *i* then becomes $$p(\theta\vert y_{-i}) = p(\theta)/p(y_i\vert\theta)$$, and the LOO distribution is then computed by taking the posterior simulations for $\theta$ obtained from `stan_glm` and giving each simulation a weight of $$1/p(y_i\vert\theta)$$
This set of weighed simulations is used to get predictive distribution on $y_i$, the held-out data point. Apparently, as using the raw weights can be noisy, the `loo` function smooths them before doing the computation.


```{r}
library("tidyverse")
library("here")
library("brms")
library("rstan")
library("bayesplot")
library("loo")
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

To see how this Bayesian LOO works, we need to fit some models.

## Data

Here we use *roaches* data from **rstanarm** R package. 

We want to compare the efficacy of a certain pest management system at reducing the number of roaches in urban apartments.

The experiment was as follows  -- treatment and control were applied to 160 and 104 apartments, respectively, and the outcome measurement y in each apartment was the number of roaches caught in a set of traps. Different apartments had traps for different numbers of days

y is the number of roaches caught in a set of traps
roach1 is the pre-treatment number of roaches
senior is indicator whether building was for senior citizens
exposure2 is exposure indicator and log(exposure2) should be used as weight

```{r}
roaches <- read_csv(here("data/roaches.csv"))
roaches$roach1 <- roaches$roach1 / 100
roaches
```

### Fit Poisson model

```{r}
get_prior(y ~ roach1 + treatment + senior + offset(log(exposure2)), roaches, family = poisson(link = "log"))
priors <- c(prior("normal(0, 2.5)", class =  "b"), prior("normal(0, 5)", class = "Intercept"))
fit1 <-
  brm(
    y ~ roach1 + treatment + senior + offset(log(exposure2)),
    data = roaches,
    family = poisson(link = "log"),
    prior = priors,
    chains = 3,
    file = here("models/y ~ roach1 + treatment + senior + offset(log(exposure2))")
  )
```


```{r}
summary(fit1)
```

### Using the loo package for model checking and comparison

```{r}
loo1 <- loo(fit1, save_psis = TRUE)
```

loo gives us warnings about the Pareto diagnostics, which indicate that for some observations the leave-one-out posteriors are different enough from the full posterior that importance-sampling is not able to correct the difference. 

We can see more details by printing the loo object.

```{r}
print(loo1)
```


The table shows us a summary of Pareto k diagnostic (Pareto shape parameter k estimated for each observation can be used as a measure of the observation's influence on posterior distribution of the model), which is used to assess the reliability of the estimates. 

Since we have some k > 1, we are not able to compute an estimate for the Monte Carlo standard error (SE) of the expected log predictive density (elpd_loo) and NA is displayed.

- elpd_loo is the estimated log score along with a SE

In this case the elpd_loo estimate should not be considered reliable. 

- p_loo is the estimated "effective number of parameters" in the model

Our model has 4 parameters. If we had a well-specified model we would expect the estimated effective number of parameters (p_loo) to be smaller than or similar to the total number of parameters in the model.

Here p_loo is almost 300, which is about 70 times the total number of parameters in the model, indicating severe model misspecification.

- looic is the LOO information criterion, -2 * elpd_loo and shows model deviance

More info can be found from pareto-k-diagnostic help page:
```{r}
help("pareto-k-diagnostic")
```

#### Plotting Pareto k diagnostics

This plot shows individual Pareto k values in same order as the original values.
 
```{r}
plot(loo1)
```

We can see that there seems to be a block of data that is somewhat easier to predict in region 100-150, but still there are some high k values.

### Try alternative models

```{r}
fit2 <- update(fit1, family = negbinomial())
```

```{r}
loo2 <- loo(fit2, save_psis = TRUE, cores = 2)
```

We still have one bad k.

```{r}
print(loo2)
```

```{r}
plot(loo2)
```

Each time the model is refit, one of the observations with a high k value is omitted and the LOO calculations are performed exactly for that observation. 

The results are then recombined with the approximate LOO calculations already carried out for the observations without problematic k values:

```{r}
if (any(pareto_k_values(loo2) > 0.7)) {
  fit2 <- add_criterion(fit2, "loo", reloo = TRUE)
  loo2 <- loo(fit2, save_psis = TRUE)
}
```


```{r}
print(loo2)
```

We can see that the Monte Carlo SE is small compared to the other uncertainties.

On the other hand, p_loo is over 6 and still a bit higher than the total number of parameters in the model. This indicates that there is almost certainly still some degree of model misspecification, but this is much better than the p_loo estimate for the Poisson model.


### Comparing the models on expected log predictive density


```{r}
loo_compare(loo1, loo2)
```


We can see than fit2 is better than fit1 more than three SE.


### Widely applicable information criterion (WAIC)

Other model fit evaluation criteria ("loo", "waic", "kfold", "loo_subsample", "bayes_R2" (Bayesian R-squared), "loo_R2" (LOO-adjusted R-squared), and "marglik" (log marginal likelihood)) can be added using `add_criterion` function or their aliases (see help Details).


```{r}
?add_criterion
```



# References

- Using the loo package (version >= 2.0.0)
 <https://mc-stan.org/loo/articles/loo2-example.html>
- Holdout validation and K-fold cross-validation of Stan programs with the loo package <https://mc-stan.org/loo/articles/loo2-elpd.html>
- Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC <http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf>

